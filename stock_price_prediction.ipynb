{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock Price Direction Prediction using LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Stock Price Direction Prediction using LSTM\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install required packages if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install yfinance pandas_ta keras-self-attention imbalanced-learn shap seaborn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import pandas_ta as ta\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.utils import class_weight\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "import tensorflow as tf\n",
    "from sklearn.decomposition import PCA\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Confirm that all required libraries are successfully imported\n",
    "print(\"All necessary libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stock ticker symbol and period\n",
    "ticker = 'AAPL'  # You can change this to any ticker symbol\n",
    "data = yf.download(ticker, start='2020-01-01', end='2023-01-01')\n",
    "\n",
    "# Save data to a CSV file\n",
    "data.to_csv('AAPL_historical_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input CSV file path\n",
    "input_file = \"AAPL_historical_data.csv\"\n",
    "\n",
    "# Output CSV file path\n",
    "output_file = \"AAPL_historical_data_output.csv\"\n",
    "\n",
    "# Read the CSV file, modify it, and save to a new file\n",
    "with open(input_file, 'r') as file:\n",
    "    reader = list(csv.reader(file))\n",
    "\n",
    "    # Modify the first row, first column\n",
    "    reader[0][0] = reader[0][0].replace(\"Price\", \"Date\")\n",
    "\n",
    "    # Remove 2nd and 3rd rows (index 1 and 2)\n",
    "    modified_data = [row for i, row in enumerate(reader) if i not in (1, 2)]\n",
    "\n",
    "# Write the modified data to a new file\n",
    "with open(output_file, 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(modified_data)\n",
    "\n",
    "print(\"CSV file has been modified and saved as:\", output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file directly\n",
    "data = pd.read_csv('AAPL_historical_data_output.csv', parse_dates=['Date'], index_col='Date')\n",
    "\n",
    "# Calculate additional advanced technical indicators\n",
    "\n",
    "# Existing Indicators\n",
    "data['SMA'] = ta.sma(data['Close'], length=20)\n",
    "data['EMA'] = ta.ema(data['Close'], length=20)\n",
    "data['RSI'] = ta.rsi(data['Close'], length=14)\n",
    "\n",
    "# MACD\n",
    "macd = ta.macd(data['Close'])\n",
    "if macd is not None:\n",
    "    data = data.join(macd)\n",
    "else:\n",
    "    print(\"MACD calculation returned None. Please check your data.\")\n",
    "\n",
    "# Bollinger Bands\n",
    "bbands = ta.bbands(data['Close'], length=20)\n",
    "if bbands is not None:\n",
    "    data = data.join(bbands)\n",
    "else:\n",
    "    print(\"Bollinger Bands calculation returned None. Please check your data.\")\n",
    "\n",
    "# Stochastic Oscillator\n",
    "stoch = ta.stoch(data['High'], data['Low'], data['Close'])\n",
    "if stoch is not None:\n",
    "    data = data.join(stoch)\n",
    "else:\n",
    "    print(\"Stochastic Oscillator calculation returned None. Please check your data.\")\n",
    "\n",
    "# Williams %R\n",
    "data['WILLIAMS_R'] = ta.willr(data['High'], data['Low'], data['Close'], length=14)\n",
    "\n",
    "# Chaikin Money Flow (CMF)\n",
    "data['CMF'] = ta.cmf(data['High'], data['Low'], data['Close'], data['Volume'], length=20)\n",
    "\n",
    "# Additional Technical Indicators\n",
    "# Average Directional Index (ADX)\n",
    "adx = ta.adx(data['High'], data['Low'], data['Close'], length=14)\n",
    "if adx is not None:\n",
    "    data = data.join(adx[['ADX_14']])\n",
    "else:\n",
    "    print(\"ADX calculation returned None. Please check your data.\")\n",
    "\n",
    "# Commodity Channel Index (CCI)\n",
    "data['CCI'] = ta.cci(data['High'], data['Low'], data['Close'], length=20)\n",
    "\n",
    "# On-Balance Volume (OBV)\n",
    "data['OBV'] = ta.obv(data['Close'], data['Volume'])\n",
    "\n",
    "# Money Flow Index (MFI)\n",
    "mfi = ta.mfi(data['High'], data['Low'], data['Close'], data['Volume'], length=14)\n",
    "if mfi is not None:\n",
    "    data['MFI'] = mfi.astype(float)  # Ensure MFI is float to avoid dtype warnings\n",
    "else:\n",
    "    print(\"MFI calculation returned None. Please check your data.\")\n",
    "\n",
    "# Time-based Features\n",
    "data['DayOfWeek'] = data.index.dayofweek\n",
    "data['Month'] = data.index.month\n",
    "data['Quarter'] = data.index.quarter\n",
    "\n",
    "# Encode Cyclical Features\n",
    "data['DayOfWeek_Sin'] = np.sin(2 * np.pi * data['DayOfWeek'] / 6)\n",
    "data['DayOfWeek_Cos'] = np.cos(2 * np.pi * data['DayOfWeek'] / 6)\n",
    "data['Month_Sin'] = np.sin(2 * np.pi * data['Month'] / 12)\n",
    "data['Month_Cos'] = np.cos(2 * np.pi * data['Month'] / 12)\n",
    "\n",
    "# Drop rows with NaN values resulting from indicator calculations\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze and Remove Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify outliers using Z-score\n",
    "z_scores = np.abs(stats.zscore(data['Close']))\n",
    "outliers = data[z_scores > 3]\n",
    "print(\"Outliers detected:\\n\", outliers)\n",
    "\n",
    "# Handling Outliers: Remove rows with outliers\n",
    "data = data[z_scores <= 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values (if any remain after prior steps)\n",
    "data.fillna(method='ffill', inplace=True)\n",
    "data.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Modify the Target Variable (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a 1-week ahead prediction (7 days)\n",
    "data['Future_Close'] = data['Close'].shift(-7)\n",
    "data['Target'] = (data['Future_Close'] > data['Close']).astype(int)\n",
    "data.dropna(inplace=True)  # Remove rows with NaN values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Prepare Features and Scale Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data.drop(['Future_Close', 'Target'], axis=1)\n",
    "target = data['Target']\n",
    "\n",
    "# Scale features\n",
    "scaler = MinMaxScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "scaled_features = pd.DataFrame(scaled_features, index=features.index, columns=features.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation Analysis\n",
    "corr_matrix = scaled_features.corr()\n",
    "plt.figure(figsize=(16, 12))\n",
    "sns.heatmap(corr_matrix, annot=False, cmap='coolwarm')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Do not drop any features due to high correlation\n",
    "print(\"No features are being dropped due to high correlation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Apply PCA to reduce dimensionality while retaining 95% variance\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "scaled_features_pca = pca.fit_transform(scaled_features)\n",
    "print(f\"Original number of features: {scaled_features.shape[1]}\")\n",
    "print(f\"Reduced number of features after PCA: {scaled_features_pca.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Create Sequences for LSTM Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(features, target, seq_length):\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(features) - seq_length):\n",
    "        X.append(features[i:i + seq_length])\n",
    "        y.append(target.iloc[i + seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "sequence_length = 50  # Adjust as needed\n",
    "\n",
    "# Create sequences using PCA-transformed features\n",
    "X, y = create_sequences(scaled_features_pca, target, sequence_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Split the Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(X) * 0.8)  # 80% for training, 20% for testing\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12:  Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SMOTE to oversample the minority class\n",
    "nsamples, nx = X_train.shape[0], X_train.shape[2]\n",
    "X_train_reshaped = X_train.reshape((nsamples, sequence_length * nx))\n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = sm.fit_resample(X_train_reshaped, y_train)\n",
    "\n",
    "# Reshape back to original shape\n",
    "X_train_resampled = X_train_resampled.reshape((X_train_resampled.shape[0], sequence_length, nx))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Build and Compile the LSTM Model with Bidirectional LSTM and Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(LSTM(128, return_sequences=True), input_shape=(sequence_length, nx)))\n",
    "model.add(SeqSelfAttention(attention_activation='sigmoid'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the Model with Focal Loss\n",
    "def focal_loss(gamma=2., alpha=.25):\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        epsilon = 1e-7\n",
    "        y_pred = tf.keras.backend.clip(y_pred, epsilon, 1. - epsilon)\n",
    "        pt = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "        loss = -alpha * tf.math.pow(1. - pt, gamma) * tf.math.log(pt)\n",
    "        return tf.reduce_mean(loss)\n",
    "    return focal_loss_fixed\n",
    "\n",
    "optimizer = Adam(learning_rate=0.0005)\n",
    "model.compile(optimizer=optimizer, loss=focal_loss(gamma=2., alpha=.25), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14: Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_resampled, y_train_resampled,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15: Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Predictions on Test Data\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Adjusting the Classification Threshold using ROC Curve\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Find the optimal threshold (Youden's J statistic)\n",
    "optimal_idx = np.argmax(tpr - fpr)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "print(f\"Optimal Threshold: {optimal_threshold}\")\n",
    "\n",
    "# Use optimal threshold\n",
    "y_pred = (y_pred_prob >= optimal_threshold).astype(int).flatten()\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion Matrix:\\n', cm)\n",
    "\n",
    "# Plot ROC Curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red', label='Optimal Threshold')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize Training History\n",
    "# Accuracy Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Loss Plot\n",
    "plt.figure(figsize=(14, 5))\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss During Training')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
